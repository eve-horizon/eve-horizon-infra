#!/usr/bin/env bash
# =============================================================================
# eve-infra -- Operational CLI for an Eve Horizon deployment
# =============================================================================
#
# Reads config from config/platform.yaml (relative to repo root).
# Targets the Kubernetes namespace "eve" using the repo-local kubeconfig:
#   config/kubeconfig.yaml
#
# Usage: eve-infra <command> [args]
# Run eve-infra --help for the full command list.
# =============================================================================
set -euo pipefail

# ---------------------------------------------------------------------------
# Resolve repo root (one level above this script's directory)
# ---------------------------------------------------------------------------
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
REPO_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
CONFIG_FILE="$REPO_ROOT/config/platform.yaml"
SECRETS_FILE="$REPO_ROOT/config/secrets.env"
KUBECONFIG_FILE="$REPO_ROOT/config/kubeconfig.yaml"

NAMESPACE="eve"
EVE_EFFECTIVE_CONTEXT=""

# ---------------------------------------------------------------------------
# Colors (disabled when stdout is not a terminal)
# ---------------------------------------------------------------------------
if [[ -t 1 ]]; then
  BOLD='\033[1m'
  DIM='\033[2m'
  GREEN='\033[0;32m'
  YELLOW='\033[0;33m'
  RED='\033[0;31m'
  CYAN='\033[0;36m'
  RESET='\033[0m'
else
  BOLD='' DIM='' GREEN='' YELLOW='' RED='' CYAN='' RESET=''
fi

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

die()  { echo -e "${RED}Error:${RESET} $*" >&2; exit 1; }
info() { echo -e "${CYAN}=>${RESET} $*"; }
warn() { echo -e "${YELLOW}Warning:${RESET} $*" >&2; }
ok()   { echo -e "${GREEN}OK${RESET} $*"; }

# Read a dotted key from platform.yaml using grep/sed (no yq dependency).
# Handles simple top-level and one-level-nested keys.
# Strips quotes, trailing comments, and whitespace.
# Usage: cfg_get "platform.version"  =>  0.1.28
cfg_get() {
  local key="$1"
  local raw
  if [[ "$key" == *.* ]]; then
    local parent="${key%%.*}"
    local child="${key#*.}"
    raw="$(sed -n "/^${parent}:/,/^[^ ]/{s/^  ${child}: *//p;}" "$CONFIG_FILE" | head -1)"
  else
    raw="$(sed -n "s/^${key}: *//p" "$CONFIG_FILE" | head -1)"
  fi
  # Strip inline comment (# ...), surrounding quotes, and trailing whitespace
  raw="${raw%%#*}"            # remove comment
  raw="${raw%"${raw##*[! ]}"}"  # trim trailing spaces
  raw="${raw#\"}"             # strip leading quote
  raw="${raw%\"}"             # strip trailing quote
  raw="${raw%"${raw##*[! ]}"}"  # trim again after quote removal
  echo "$raw"
}

# Resolve KUBECONFIG to the repo-local canonical file.
resolve_kubeconfig() {
  local target="$KUBECONFIG_FILE"

  if [[ ! -f "$target" ]]; then
    die "Missing kubeconfig: $target
Generate it with:
  eve-infra kubeconfig refresh
Then verify with:
  eve-infra kubeconfig doctor"
  fi

  # Ignore external KUBECONFIG to avoid cross-repo context contamination.
  if [[ -n "${KUBECONFIG:-}" && "$KUBECONFIG" != "$target" ]]; then
    warn "Ignoring external KUBECONFIG='$KUBECONFIG' and using canonical '$target'."
  fi
  export KUBECONFIG="$target"
}

require_kubectl() {
  command -v kubectl &>/dev/null || die "kubectl is not installed."
}

require_aws() {
  command -v aws &>/dev/null || die "aws CLI is required (for kubeconfig refresh)."
}

require_config() {
  [[ -f "$CONFIG_FILE" ]] || die "Config not found at $CONFIG_FILE"
}

current_kube_context() {
  local args=()
  [[ -n "${KUBECONFIG:-}" ]] && args+=(--kubeconfig "$KUBECONFIG")
  kubectl "${args[@]}" config current-context 2>/dev/null || true
}

assert_safe_kube_context() {
  local intent="${1:-read}"

  if [[ "${EVE_KUBE_GUARD_BYPASS:-0}" == "1" ]]; then
    warn "Bypassing kube context guard for ${intent} operations (EVE_KUBE_GUARD_BYPASS=1)."
    return
  fi

  local cloud compute_model prefix ctx expected_cluster
  cloud="$(cfg_get 'cloud')"
  compute_model="$(cfg_get 'compute.model')"
  prefix="$(cfg_get 'name_prefix')"
  ctx="$(current_kube_context)"

  [[ -n "$ctx" ]] || die "kubectl has no current context. Configure kube access before running eve-infra."

  expected_cluster="${prefix}-cluster"

  # AWS EKS: enforce explicit cluster match.
  if [[ "$cloud" == "aws" && "$compute_model" == "eks" ]]; then
    if [[ "$ctx" != *":cluster/${expected_cluster}" && "$ctx" != "$expected_cluster" ]]; then
      die "Unsafe kube context '$ctx' for aws/eks. Expected context for '${expected_cluster}'.
Switch context first or intentionally override with EVE_KUBE_GUARD_BYPASS=1."
    fi
    EVE_EFFECTIVE_CONTEXT="$ctx"
    return
  fi

  # Non-EKS paths: block obvious EKS contexts.
  if [[ "$ctx" == arn:aws:eks:* || "$ctx" == *":cluster/"* ]]; then
    die "Unsafe kube context '$ctx' for cloud='$cloud' compute.model='${compute_model:-unset}'.
Switch context first or intentionally override with EVE_KUBE_GUARD_BYPASS=1."
  fi

  EVE_EFFECTIVE_CONTEXT="$ctx"
}

kube() {
  local args=()
  [[ -n "${KUBECONFIG:-}" ]] && args+=(--kubeconfig "$KUBECONFIG")
  [[ -n "${EVE_EFFECTIVE_CONTEXT:-}" ]] && args+=(--context "$EVE_EFFECTIVE_CONTEXT")
  kubectl "${args[@]}" "$@"
}

require_cluster_access() {
  local intent="${1:-read}"
  require_config
  require_kubectl
  resolve_kubeconfig
  assert_safe_kube_context "$intent"
}

# Map user-friendly service names to k8s resource names.
# agent-runtime is a StatefulSet; everything else is a Deployment.
declare -A SERVICE_MAP=(
  [api]="deployment/eve-api"
  [worker]="deployment/eve-worker"
  [orchestrator]="deployment/eve-orchestrator"
  [gateway]="deployment/eve-gateway"
  [agent-runtime]="statefulset/eve-agent-runtime"
  [buildkitd]="deployment/buildkitd"
)

declare -A SERVICE_LABELS=(
  [api]="app.kubernetes.io/name=eve-api"
  [worker]="app.kubernetes.io/name=eve-worker"
  [orchestrator]="app.kubernetes.io/name=eve-orchestrator"
  [gateway]="app.kubernetes.io/name=eve-gateway"
  [agent-runtime]="app.kubernetes.io/name=eve-agent-runtime"
  [buildkitd]="app.kubernetes.io/name=buildkitd"
)

VALID_SERVICES="api, worker, orchestrator, gateway, agent-runtime"

resolve_service() {
  local svc="${1:-}"
  [[ -n "$svc" ]] || die "Service name required. Valid: $VALID_SERVICES"
  [[ -n "${SERVICE_MAP[$svc]:-}" ]] || die "Unknown service '$svc'. Valid: $VALID_SERVICES"
  echo "${SERVICE_MAP[$svc]}"
}

resolve_label() {
  local svc="${1:-}"
  [[ -n "${SERVICE_LABELS[$svc]:-}" ]] || die "Unknown service '$svc'. Valid: $VALID_SERVICES"
  echo "${SERVICE_LABELS[$svc]}"
}

# ---------------------------------------------------------------------------
# Commands
# ---------------------------------------------------------------------------

cmd_status() {
  require_cluster_access read

  local version name_prefix env cloud
  version="$(cfg_get 'platform.version')"
  name_prefix="$(cfg_get 'name_prefix')"
  env="$(cfg_get 'environment')"
  cloud="$(cfg_get 'cloud')"
  local api_host
  api_host="$(cfg_get 'api_host')"

  echo ""
  echo -e "${BOLD}Eve Horizon Platform Status${RESET}"
  echo -e "  Version:     ${GREEN}${version}${RESET}"
  echo -e "  Environment: ${env}"
  echo -e "  Cloud:       ${cloud}"
  echo -e "  Name prefix: ${name_prefix}"
  echo -e "  API host:    ${api_host}"
  echo -e "  Namespace:   ${NAMESPACE}"
  local kube_ctx
  kube_ctx="$(current_kube_context)"
  [[ -n "$kube_ctx" ]] && echo -e "  Context:     ${DIM}${kube_ctx}${RESET}"
  if [[ -n "${KUBECONFIG:-}" ]]; then
    echo -e "  Kubeconfig:  ${DIM}${KUBECONFIG}${RESET}"
  fi
  echo ""

  echo -e "${BOLD}Pods${RESET}"
  kube get pods -n "$NAMESPACE" -o wide 2>/dev/null || warn "Could not reach cluster."
  echo ""

  echo -e "${BOLD}Services${RESET}"
  kube get svc -n "$NAMESPACE" 2>/dev/null || true
  echo ""
}

cmd_version() {
  require_config
  local version registry
  version="$(cfg_get 'platform.version')"
  registry="$(cfg_get 'platform.registry')"

  echo -e "${BOLD}Current version:${RESET} ${GREEN}${version}${RESET}"
  echo -e "${BOLD}Registry:${RESET}        ${registry}"
  echo ""

  info "Checking for latest version at ${registry}..."
  local registry_host registry_path tags latest response
  registry_host="${registry%%/*}"
  registry_path="${registry#*/}"
  tags=""

  case "$registry_host" in
    ghcr.io)
      if ! command -v curl &>/dev/null || ! command -v jq &>/dev/null; then
        warn "curl + jq are required to query GHCR tags."
        return
      fi
      response="$(curl -sfL "https://ghcr.io/v2/${registry_path}/api/tags/list" 2>/dev/null || true)"
      tags="$(echo "$response" | jq -r '.tags[]? // empty' 2>/dev/null || true)"
      ;;
    public.ecr.aws)
      if ! command -v curl &>/dev/null || ! command -v jq &>/dev/null; then
        warn "curl + jq are required to query public ECR tags."
        return
      fi
      local registry_alias registry_namespace repository token tags_json
      registry_alias="${registry_path%%/*}"
      registry_namespace="${registry_path#*/}"
      if [[ -z "$registry_alias" || -z "$registry_namespace" || "$registry_namespace" == "$registry_path" ]]; then
        warn "Invalid public ECR registry path '${registry}'. Expected public.ecr.aws/<alias>/<namespace>."
        return
      fi
      repository="${registry_namespace}/api"
      token="$(curl -sfL "https://public.ecr.aws/token/?service=public.ecr.aws&scope=repository:${repository}:pull" \
        | jq -r '.token // empty' 2>/dev/null || true)"
      [[ -n "$token" ]] || return
      tags_json="$(curl -sfL -H "Authorization: Bearer ${token}" \
        "https://public.ecr.aws/v2/${registry_alias}/${repository}/tags/list" 2>/dev/null || true)"
      tags="$(echo "$tags_json" | jq -r '.tags[]? // empty' 2>/dev/null || true)"
      ;;
    *.dkr.ecr.*.amazonaws.com)
      if ! command -v aws &>/dev/null; then
        warn "aws CLI is required to query private ECR tags."
        return
      fi
      local region
      region="$(cfg_get 'region')"
      region="${region:-us-east-1}"
      tags="$(aws ecr describe-images \
        --repository-name "${registry_path}/api" \
        --region "$region" \
        --query 'imageDetails[].imageTags[]' \
        --output text 2>/dev/null \
        | tr '\t' '\n' \
        | tr ' ' '\n' \
        | sed '/^None$/d;/^$/d' \
        || true)"
      ;;
    *)
      warn "Unsupported registry host '${registry_host}'."
      return
      ;;
  esac

  latest="$(echo "$tags" \
    | grep -E '^[0-9]+\.[0-9]+\.[0-9]+$' \
    | sort -V | tail -1 || true)"

  if [[ -z "$latest" ]]; then
    echo -e "${DIM}Could not determine latest version from registry.${RESET}"
    return
  fi

  if [[ "$latest" == "$version" ]]; then
    ok "You are on the latest version."
  else
    echo -e "${YELLOW}Latest available:${RESET} ${latest}"
    echo "  Run: eve-infra upgrade $latest"
  fi
}

cmd_kubeconfig_doctor() {
  require_config
  require_kubectl
  resolve_kubeconfig

  local cloud compute_model prefix ctx expected_cluster
  cloud="$(cfg_get 'cloud')"
  compute_model="$(cfg_get 'compute.model')"
  prefix="$(cfg_get 'name_prefix')"
  expected_cluster="${prefix}-cluster"
  ctx="$(current_kube_context)"

  [[ -n "$ctx" ]] || die "kubeconfig exists but has no current context: $KUBECONFIG"
  assert_safe_kube_context read

  info "Validating Kubernetes API connectivity..."
  kube cluster-info >/dev/null 2>&1 || die "Unable to reach cluster with $KUBECONFIG (context: $ctx)"

  ok "Kubeconfig is healthy."
  echo ""
  echo "  Kubeconfig:  $KUBECONFIG"
  echo "  Context:     $ctx"
  if [[ "$cloud" == "aws" && "$compute_model" == "eks" ]]; then
    echo "  Expected:    ${expected_cluster} (or arn:...:cluster/${expected_cluster})"
  fi
}

cmd_kubeconfig_refresh() {
  require_config
  require_kubectl

  local cloud compute_model
  cloud="$(cfg_get 'cloud')"
  compute_model="$(cfg_get 'compute.model')"

  mkdir -p "$(dirname "$KUBECONFIG_FILE")"

  if [[ "$cloud" == "aws" && "$compute_model" == "eks" ]]; then
    require_aws
    local prefix cluster_name region
    prefix="$(cfg_get 'name_prefix')"
    cluster_name="${prefix}-cluster"
    region="$(cfg_get 'region')"
    [[ -n "$region" ]] || die "region is not set in config/platform.yaml"

    info "Refreshing kubeconfig for EKS cluster '${cluster_name}' in region '${region}'..."
    aws eks update-kubeconfig \
      --name "$cluster_name" \
      --region "$region" \
      --kubeconfig "$KUBECONFIG_FILE" \
      --alias "$cluster_name" >/dev/null
    export KUBECONFIG="$KUBECONFIG_FILE"

    # Ensure current-context points at the expected alias.
    kubectl --kubeconfig "$KUBECONFIG_FILE" config use-context "$cluster_name" >/dev/null 2>&1 || true

    ok "Wrote kubeconfig to $KUBECONFIG_FILE"
    cmd_kubeconfig_doctor
    return
  fi

  die "kubeconfig refresh is only implemented for cloud=aws + compute.model=eks.
For this deployment, write kubeconfig manually to:
  $KUBECONFIG_FILE"
}

cmd_kubeconfig() {
  local sub="${1:-}"
  shift 2>/dev/null || true

  case "$sub" in
    refresh) cmd_kubeconfig_refresh "$@" ;;
    doctor)  cmd_kubeconfig_doctor "$@" ;;
    *)
      die "Usage: eve-infra kubeconfig <refresh|doctor>"
      ;;
  esac
}

cmd_upgrade() {
  local new_version="${1:-}"
  [[ -n "$new_version" ]] || die "Usage: eve-infra upgrade <version>"
  require_config

  local old_version
  old_version="$(cfg_get 'platform.version')"
  local registry
  registry="$(cfg_get 'platform.registry')"
  local overlay
  overlay="$(cfg_get 'overlay')"
  [[ -z "$overlay" ]] && overlay="$(cfg_get 'cloud')"

  if [[ "$old_version" == "$new_version" ]]; then
    ok "Already at version ${new_version}. Nothing to do."
    return
  fi

  info "Upgrading: ${old_version} -> ${new_version}"

  # 1. Update platform.yaml
  info "Updating config/platform.yaml..."
  sed -i.bak "s/version: *\"${old_version}\"/version: \"${new_version}\"/" "$CONFIG_FILE"
  rm -f "$CONFIG_FILE.bak"

  # 2. Update image tags in overlay patches
  local overlay_dir="$REPO_ROOT/k8s/overlays/${overlay}"
  if [[ -d "$overlay_dir" ]]; then
    info "Updating image tags in k8s/overlays/${overlay}/..."
    local patch_file
    for patch_file in "$overlay_dir"/*-patch.yaml; do
      [[ -f "$patch_file" ]] || continue
      # Normalize service image host + tag to configured registry and target version.
      local service
      for service in api gateway orchestrator worker agent-runtime sso; do
        perl -0pi -e "s|image:\\s*[^\\s\"']*/${service}:[^\\s\"']+|image: ${registry}/${service}:${new_version}|g" "$patch_file"
      done
      perl -0pi -e "s|value:\\s*\"?[^\"\\s]*/worker(-full)?:[^\"\\s]+\"?|value: \"${registry}/worker\\1:${new_version}\"|g" "$patch_file"
      rm -f "$patch_file.bak"
    done
  else
    warn "Overlay directory not found: $overlay_dir (skipping patch update)"
  fi

  # 3. Offer to commit
  ok "Files updated to version ${new_version}."
  echo ""
  echo "  Review changes:"
  echo "    git diff"
  echo ""
  echo "  Commit and deploy:"
  echo "    git add -A && git commit -m \"chore: upgrade eve platform to ${new_version}\""
  echo "    eve-infra deploy"
}

cmd_deploy() {
  require_cluster_access write

  local overlay
  overlay="$(cfg_get 'overlay')"
  [[ -z "$overlay" ]] && overlay="$(cfg_get 'cloud')"
  local overlay_dir="$REPO_ROOT/k8s/overlays/${overlay}"

  [[ -d "$overlay_dir" ]] || die "Overlay directory not found: $overlay_dir"

  info "Building manifests with kustomize (overlay: ${overlay})..."
  if command -v kustomize &>/dev/null; then
    kustomize build "$overlay_dir" | kube apply -f - -n "$NAMESPACE"
  else
    kube apply -k "$overlay_dir" -n "$NAMESPACE"
  fi

  echo ""
  info "Waiting for rollouts..."
  local resource
  for resource in \
    deployment/eve-api \
    deployment/eve-worker \
    deployment/eve-orchestrator \
    deployment/eve-gateway \
    statefulset/eve-agent-runtime; do
    echo -n "  ${resource}... "
    if kube rollout status "$resource" -n "$NAMESPACE" --timeout=120s 2>/dev/null; then
      echo -e "${GREEN}ready${RESET}"
    else
      echo -e "${RED}timed out${RESET}"
    fi
  done

  echo ""
  ok "Deploy complete."
}

cmd_secrets_sync() {
  require_cluster_access write

  if [[ ! -f "$SECRETS_FILE" ]]; then
    die "Secrets file not found at ${SECRETS_FILE}.
  Create it from the example:
    cp config/secrets.env.example config/secrets.env"
  fi

  info "Syncing secrets from config/secrets.env -> k8s secret 'eve-app' in namespace '${NAMESPACE}'..."

  kube create secret generic eve-app \
    --from-env-file="$SECRETS_FILE" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml \
    | kube apply -f -

  ok "Secret 'eve-app' updated."
  echo ""
  echo "  Services will pick up new values on next restart:"
  echo "    eve-infra restart api"
  echo "    eve-infra restart worker"
}

cmd_secrets_show() {
  require_cluster_access read

  info "Configured keys in secret 'eve-app':"
  echo ""
  kube get secret eve-app -n "$NAMESPACE" -o json 2>/dev/null \
    | jq -r '.data | keys[]' 2>/dev/null \
    | sort \
    || warn "Could not read secret 'eve-app'. It may not exist yet."
}

cmd_db_migrate() {
  require_cluster_access write

  local job_name="eve-db-migrate"

  # Delete previous completed/failed job if it exists (jobs are immutable)
  if kube get job "$job_name" -n "$NAMESPACE" &>/dev/null; then
    info "Deleting previous migration job..."
    kube delete job "$job_name" -n "$NAMESPACE" --ignore-not-found
  fi

  local overlay
  overlay="$(cfg_get 'overlay')"
  [[ -z "$overlay" ]] && overlay="$(cfg_get 'cloud')"
  local overlay_dir="$REPO_ROOT/k8s/overlays/${overlay}"

  info "Applying migration job..."
  if [[ -d "$overlay_dir" ]]; then
    # Build with kustomize to get the overlay-patched job (correct image + DATABASE_URL)
    if command -v kustomize &>/dev/null; then
      kustomize build "$overlay_dir" \
        | kube apply -f - -n "$NAMESPACE" --server-side --field-manager=eve-infra \
        -l app.kubernetes.io/name=eve-db-migrate 2>/dev/null \
        || kustomize build "$overlay_dir" | kube apply -f - -n "$NAMESPACE"
    else
      kube apply -k "$overlay_dir" -n "$NAMESPACE"
    fi
  else
    kube apply -f "$REPO_ROOT/k8s/base/db-migrate-job.yaml" -n "$NAMESPACE"
  fi

  info "Waiting for migration to complete (timeout: 120s)..."
  if kube wait --for=condition=complete "job/$job_name" -n "$NAMESPACE" --timeout=120s 2>/dev/null; then
    ok "Migration complete."
    echo ""
    echo "  Logs:"
    kube logs "job/$job_name" -n "$NAMESPACE" --tail=20 2>/dev/null || true
  else
    echo -e "${RED}Migration did not complete within timeout.${RESET}"
    echo ""
    echo "  Check logs:"
    echo "    kubectl --kubeconfig $KUBECONFIG_FILE -n $NAMESPACE logs job/$job_name"
    exit 1
  fi
}

cmd_db_backup() {
  require_config
  local provider
  provider="$(cfg_get 'database.provider')"

  echo -e "${BOLD}Database Backup${RESET}"
  echo ""

  case "$provider" in
    rds)
      info "Database provider: AWS RDS"
      echo "  RDS automated backups are managed by AWS."
      echo ""
      echo "  To create a manual snapshot:"
      echo "    aws rds create-db-snapshot \\"
      echo "      --db-instance-identifier <instance-id> \\"
      echo "      --db-snapshot-identifier eve-backup-\$(date +%Y%m%d-%H%M%S)"
      echo ""
      echo "  To list existing snapshots:"
      echo "    aws rds describe-db-snapshots --db-instance-identifier <instance-id>"
      ;;
    external)
      info "Database provider: external"
      echo "  Use pg_dump against your external database."
      echo "    pg_dump \$DATABASE_URL > eve-backup-\$(date +%Y%m%d-%H%M%S).sql"
      ;;
    in-cluster)
      info "Database provider: in-cluster PostgreSQL"
      echo "  Run pg_dump inside the postgres pod:"
      echo "    kubectl exec -n $NAMESPACE postgres-0 -- pg_dump -U eve eve > backup.sql"
      ;;
    *)
      warn "Unknown database provider: $provider"
      ;;
  esac
}

cmd_db_connect() {
  require_cluster_access write

  local provider
  provider="$(cfg_get 'database.provider')"
  local db_name
  db_name="$(cfg_get 'database.name')"
  [[ -z "$db_name" ]] && db_name="eve"
  local db_user
  db_user="$(cfg_get 'database.username')"
  [[ -z "$db_user" ]] && db_user="eve"

  case "$provider" in
    in-cluster)
      info "Connecting to in-cluster PostgreSQL..."
      kube exec -it -n "$NAMESPACE" postgres-0 -- psql -U "$db_user" "$db_name"
      ;;
    rds|cloud-sql|external)
      # Port-forward through the API pod (which has DATABASE_URL) as a SOCKS proxy
      # is complex; instead, run psql in a temporary pod with the same secret.
      info "Launching psql in a temporary pod (using eve-app secret for DATABASE_URL)..."
      echo -e "${DIM}Press Ctrl+D or type \\q to exit.${RESET}"
      echo ""
      kube run eve-psql-session \
        --rm -it \
        --image=postgres:16-alpine \
        --namespace="$NAMESPACE" \
        --restart=Never \
        --env="PGDATABASE=$db_name" \
        --overrides='{
          "spec": {
            "containers": [{
              "name": "psql",
              "image": "postgres:16-alpine",
              "stdin": true,
              "tty": true,
              "command": ["psql"],
              "args": ["$(DATABASE_URL)"],
              "envFrom": [{"secretRef": {"name": "eve-app"}}]
            }]
          }
        }' \
        -- psql 2>/dev/null \
        || die "Failed to launch psql session. Ensure the eve-app secret contains DATABASE_URL."
      ;;
    *)
      die "Unknown database provider: $provider"
      ;;
  esac
}

cmd_logs() {
  local service="${1:-}"
  [[ -n "$service" ]] || die "Usage: eve-infra logs <service>
  Services: $VALID_SERVICES"

  require_cluster_access read

  local label
  label="$(resolve_label "$service")"
  shift

  info "Tailing logs for ${service} (${label})..."
  kube logs -f -l "$label" -n "$NAMESPACE" --all-containers --prefix "$@"
}

cmd_restart() {
  local service="${1:-}"
  [[ -n "$service" ]] || die "Usage: eve-infra restart <service>
  Services: $VALID_SERVICES"

  require_cluster_access write

  local resource
  resource="$(resolve_service "$service")"

  info "Rolling restart: ${resource}..."
  kube rollout restart "$resource" -n "$NAMESPACE"

  echo -n "  Waiting for rollout... "
  if kube rollout status "$resource" -n "$NAMESPACE" --timeout=120s 2>/dev/null; then
    echo -e "${GREEN}ready${RESET}"
  else
    echo -e "${RED}timed out${RESET}"
    exit 1
  fi

  ok "${service} restarted."
}

cmd_health() {
  require_config

  local api_host
  api_host="$(cfg_get 'api_host')"
  [[ -n "$api_host" ]] || die "api_host not set in config."

  local url="https://${api_host}/health"
  info "Checking ${url}..."

  if ! command -v curl &>/dev/null; then
    die "curl is required for health checks."
  fi

  local http_code body
  http_code="$(curl -s -o /dev/null -w '%{http_code}' --max-time 10 "$url" 2>/dev/null || echo "000")"
  body="$(curl -sf --max-time 10 "$url" 2>/dev/null || true)"

  if [[ "$http_code" == "200" ]]; then
    ok "API is healthy (HTTP $http_code)"
    if [[ -n "$body" ]] && command -v jq &>/dev/null; then
      echo "$body" | jq . 2>/dev/null || echo "$body"
    elif [[ -n "$body" ]]; then
      echo "$body"
    fi
  else
    echo -e "${RED}UNHEALTHY${RESET} (HTTP $http_code)"
    [[ -n "$body" ]] && echo "$body"
    echo ""
    echo "  Troubleshoot:"
    echo "    eve-infra status"
    echo "    eve-infra logs api"
    exit 1
  fi
}

# ---------------------------------------------------------------------------
# Help
# ---------------------------------------------------------------------------

show_help() {
  cat <<'HELP'
Eve Horizon Infrastructure CLI

Usage: eve-infra <command> [args]

Platform:
  status              Show platform version, service health, pod status
  version             Show current + latest available version
  kubeconfig doctor   Validate config/kubeconfig.yaml and cluster connectivity
  kubeconfig refresh  Regenerate config/kubeconfig.yaml (aws/eks)
  upgrade <version>   Update platform.yaml, patch overlays for new version
  deploy              Apply current config to cluster (kustomize build | kubectl apply)
  health              Run health check against the API endpoint

Secrets:
  secrets sync        Push config/secrets.env to k8s eve-app secret
  secrets show        List which secret keys are configured

Database:
  db migrate          Run the database migration job
  db backup           Show backup instructions for your database provider
  db connect          Open an interactive psql session

Operations:
  logs <service>      Tail logs for a service
  restart <service>   Rolling restart of a service

Services: api, worker, orchestrator, gateway, agent-runtime

Environment:
  Canonical kubeconfig:
                      config/kubeconfig.yaml (required, repo-local)
  KUBECONFIG          Ignored if set to a different path (safety hardening)
  EVE_KUBE_GUARD_BYPASS=1
                      Emergency override to bypass context safety checks
  Config file:        config/platform.yaml (relative to repo root)
  Secrets file:       config/secrets.env (not committed; see secrets.env.example)

Examples:
  eve-infra status
  eve-infra kubeconfig doctor
  eve-infra kubeconfig refresh
  eve-infra upgrade 0.2.0
  eve-infra deploy
  eve-infra secrets sync
  eve-infra logs api
  eve-infra restart worker
  eve-infra db migrate
  eve-infra health
HELP
}

# ---------------------------------------------------------------------------
# Main dispatcher
# ---------------------------------------------------------------------------

main() {
  local command="${1:-}"
  if [[ -z "$command" || "$command" == "--help" || "$command" == "-h" ]]; then
    show_help
    exit 0
  fi
  shift

  case "$command" in
    status)   cmd_status "$@" ;;
    version)  cmd_version "$@" ;;
    kubeconfig) cmd_kubeconfig "$@" ;;
    upgrade)  cmd_upgrade "$@" ;;
    deploy)   cmd_deploy "$@" ;;
    health)   cmd_health "$@" ;;
    logs)     cmd_logs "$@" ;;
    restart)  cmd_restart "$@" ;;

    secrets)
      local sub="${1:-}"
      shift 2>/dev/null || true
      case "$sub" in
        sync) cmd_secrets_sync "$@" ;;
        show) cmd_secrets_show "$@" ;;
        *)    die "Usage: eve-infra secrets <sync|show>" ;;
      esac
      ;;

    db)
      local sub="${1:-}"
      shift 2>/dev/null || true
      case "$sub" in
        migrate) cmd_db_migrate "$@" ;;
        backup)  cmd_db_backup "$@" ;;
        connect) cmd_db_connect "$@" ;;
        *)       die "Usage: eve-infra db <migrate|backup|connect>" ;;
      esac
      ;;

    *)
      echo -e "${RED}Unknown command:${RESET} $command"
      echo ""
      show_help
      exit 1
      ;;
  esac
}

main "$@"
